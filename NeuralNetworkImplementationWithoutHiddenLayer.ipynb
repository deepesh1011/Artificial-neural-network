{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XOR function \n",
    "# X is input data matrix where each row is training example\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "# Y is output data matrix where each row is training example\n",
    "Y = np.array([[0,0,0,1]]).T\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sigmoid activation function \n",
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.9671113],\n",
       "        [-0.9183824]]), array([-0.34937495]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random set of initial weights and bias\n",
    "weights=2*np.random.random((2,1))-1\n",
    "bias=2*np.random.random(1)-1\n",
    "lr=.1 #Learning rate\n",
    "weights,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.19897464e-04]\n",
      " [  6.40480830e-02]\n",
      " [  6.40483761e-02]\n",
      " [  9.36036087e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 5.36526497],\n",
       "        [ 5.36526008]]), array([-8.04719009]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building neural network which does not have hidden layer\n",
    "for iter in range(3000):\n",
    "    output0=X\n",
    "    #Doing forward propagation so get the predicted output value Yp \n",
    "    output=sig(np.dot(output0,weights)+bias)\n",
    "    input_for_last_layer=np.dot(output0,weights)+bias\n",
    "    #Output sum margin of error(first term)=Calculated - Target\n",
    "    first_term=output-Y\n",
    "    second_term=derivativeSig(input_for_last_layer) \n",
    "    first_two=first_term*second_term\n",
    "\n",
    "    #Going through each of the feature using corresponding weights that should give the gradient\n",
    "    #Multiplying W11 to feature1 and W12 to feature2 using vector multiplication\n",
    "    #Derivative of sigmoid will give us gradient of activation funciton \n",
    "    changes+=np.dot(output0.T,first_two)\n",
    "    #Update weights\n",
    "    weights=weights-lr*changes\n",
    "    bias_change=0.0\n",
    "    #Update bias\n",
    "    bias_change+=np.sum(first_two) \n",
    "    bias=bias-lr*bias_change \n",
    "#Final output    \n",
    "output=sig(np.dot(X,weights)+bias)  \n",
    "print(output)\n",
    "weights,bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
